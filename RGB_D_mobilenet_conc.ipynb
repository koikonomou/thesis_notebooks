{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.image import imread\n",
    "import os\n",
    "import sys\n",
    "if '/opt/ros/kinetic/lib/python2.7/dist-packages' in sys.path: sys.path.remove('/opt/ros/melodic/lib/python2.7/dist-packages')\n",
    "import cv2 \n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# try:\n",
    "#     tf.config.experimental.set_memory_growth((tf.config.experimental.list_physical_devices('GPU'))[0], True)\n",
    "# except:\n",
    "#     pass\n",
    "from math import pi\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definitions\n",
    "DATA_DIR = \"/home/kate/Real_Dataset/conc_imgs/\"\n",
    "DEPTH_FILE, RGB_FILE, SEGMENT_FILE = \"Depth.png\", \"RGB.png\", \"Segments.png\"\n",
    "NUM_SEGMENTS = 9 # 10 needed, 2 additional\n",
    "SEGMENT_NAMES = [\"_background_\", \"table\", \"cup\", \"bottle\", \"glass\", \"fork\", \"knife\", \"food\", \"plate\"]\n",
    "\n",
    "# Camera information\n",
    "IMG_HEIGHT, IMG_WIDTH, CHANNELS = 720, 2560, 4 # Shape of the images\n",
    "OUT_SHAPE = (720, 2560)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __getRGB(thisPath): \n",
    "#     \"\"\"Internal: tf-pure function to return the RGB Image of a datatpoint\"\"\"\n",
    "    png = tf.io.read_file(thisPath, name=\"read_file_RGB\")\n",
    "    png = tf.image.decode_png(png, name=\"RGB_Decoding\")\n",
    "    png = tf.image.convert_image_dtype(png, tf.float32, name=\"RGB_RangeChange\")\n",
    "    png.set_shape((IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "    return png\n",
    "\n",
    "def __getD(thisPath): \n",
    "#      \"\"\"Internal: tf-pure function to return the Depth Image of a datatpoint\"\"\"\n",
    "    D = tf.io.read_file(thisPath, name=\"read_file_Depth\")\n",
    "    D = tf.image.decode_png(D, dtype=tf.uint16, name=\"Depth_Decoding\")\n",
    "    D = tf.cast(D, tf.float32, name=\"Depth_uint16_to_float32\")\n",
    "    D = tf.math.reciprocal_no_nan(D, name=\"Invert_Depth\")\n",
    "    D.set_shape((IMG_HEIGHT, IMG_WIDTH, 1))\n",
    "    return D\n",
    "\n",
    "def __getSegments(thisPath): \n",
    "#     \"\"\"Internal: tf-pure function to return the Segment Image-Set of a datatpoint\"\"\"\n",
    "    S = tf.io.read_file(thisPath, name=\"read_file_Segments\")\n",
    "    S = tf.image.decode_png(S, name=\"Segments_Decoding\")[:,:,0]\n",
    "    S = tf.squeeze(S, name=\"Segments_Squeezing\")\n",
    "    S = tf.one_hot(S, NUM_SEGMENTS, name=\"One_Hot_Segments\")\n",
    "    S.set_shape((IMG_HEIGHT, IMG_WIDTH, NUM_SEGMENTS))\n",
    "#     S = tf.image.resize(S, size=OUT_SHAPE)\n",
    "    return S\n",
    "   \n",
    "@tf.function\n",
    "\n",
    "def loadDatapoint(paths):\n",
    "#     \"\"\"Loads a single datapoint and returns the (4+NUM_SEGMENTS+3) Channel x-Image and the label\"\"\"\n",
    "    RGB = __getRGB(paths+\"/\"+RGB_FILE)\n",
    "    D = __getD(paths+\"/\"+DEPTH_FILE)\n",
    "    S = __getSegments(paths+\"/\"+SEGMENT_FILE)\n",
    "    x = tf.concat([RGB,D], axis=2, name=\"Stack_RGB_D\")\n",
    "    return x, S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showDatapoint(x,y):\n",
    "    \"\"\"Represent a single datapoint in ipynb-readable format\"\"\"\n",
    "    image, label = x, y \n",
    "    img = image.numpy()\n",
    "    C = img[:,:,:3]\n",
    "    D = img[:,:,3]\n",
    "    S = label\n",
    "    plt.figure(\"Images\", figsize=(30,2))\n",
    "    plt.subplot(1,3,1)\n",
    "    I_C= plt.imshow(C)\n",
    "    plt.title(\"RGB\")\n",
    "    plt.subplot(1,3,2)\n",
    "    I_D=plt.imshow(D)\n",
    "    plt.title(\"Depth$^{-1}$\")\n",
    "    for i in (I_C,I_D):\n",
    "        i.axes.get_yaxis().set_visible(False)\n",
    "        i.axes.get_xaxis().set_visible(False)\n",
    "    plt.show()\n",
    "    f1 = plt.figure(\"Segments\", figsize=(30, 2))\n",
    "    f1.suptitle(\"Segments\")\n",
    "    for i in range(NUM_SEGMENTS):\n",
    "        s = S[:,:,i]\n",
    "        plt.subplot(1,NUM_SEGMENTS,i+1)\n",
    "        if i < len(SEGMENT_NAMES):\n",
    "            plt.title(SEGMENT_NAMES[i])\n",
    "        else: plt.title(\"Placeholder\")\n",
    "        I_S = plt.imshow(s, vmin=0, vmax=1)\n",
    "        I_S.axes.get_yaxis().set_visible(False)\n",
    "        I_S.axes.get_xaxis().set_visible(False)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataset(datasetPath, batch_size=None, batch_drop_remainder=False, cache=None, prefetch_buffer_size=tf.data.experimental.AUTOTUNE, testPercentage=20, random_seed=13):\n",
    "#    \"\"\"Returns the seed-defined shuffled dataset from the csv as train_set, test_set\n",
    "#         Parameters:\n",
    "#         datasetPath: The path to the dataset\n",
    "#         batch_size: batch size to run with\n",
    "#         cache: From the dataset doku: filename: \n",
    "#             A tf.string scalar tf.Tensor, representing the name of a directory on the filesystem to \n",
    "#             use for caching elements in this Dataset. If a filename is not provided, the dataset will \n",
    "#             be cached in memory.\n",
    "#         prefetch_buffer_size: buffer size used in prefetching\n",
    "#         testPercentage: Percentage of the data used for testing\n",
    "#         random_seed: seed used by numpy to shuffle the dataset \"\"\"\n",
    "\n",
    "    DATASET_LENGTH = 224\n",
    "    testPercentage = 10  # Percentage of data used for testing\n",
    "    dsSplit = int(DATASET_LENGTH*testPercentage/100.) # Number of items in training set\n",
    "    paths = [datasetPath+str(i)+\"/\" for i in range(1,100) if not 0 ]\n",
    "    np.random_seed=random_seed\n",
    "    np.random.shuffle(paths)\n",
    "    ds = tf.data.Dataset.from_tensor_slices((paths))\n",
    "    ds = ds.map(loadDatapoint, num_parallel_calls=tf.data.experimental.AUTOTUNE) \n",
    "    if cache:\n",
    "        ds = ds.cache(cache)\n",
    "    if batch_size:\n",
    "        ds = ds.batch(batch_size, drop_remainder=batch_drop_remainder)\n",
    "    if prefetch_buffer_size:\n",
    "        ds = ds.prefetch(buffer_size=prefetch_buffer_size)\n",
    "    return ds.skip(dsSplit), ds.take(dsSplit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    dsTrain, dsTest = getDataset(datasetPath = \n",
    "                             DATA_DIR, \n",
    "                             batch_size=4)#, batch_drop_remainder=True, cache=None)#\"~\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Incompatible shapes at component 0: expected [?,720,2560,4] but got [4,720,2560,6].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mexecution_mode\u001b[0;34m(mode)\u001b[0m\n\u001b[1;32m   2112\u001b[0m       \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecutor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecutor_new\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2113\u001b[0;31m       \u001b[0;32myield\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2114\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    732\u001b[0m           \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 733\u001b[0;31m           output_shapes=self._flat_output_shapes)\n\u001b[0m\u001b[1;32m    734\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   2578\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2579\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2580\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6861\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6862\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6863\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Incompatible shapes at component 0: expected [?,720,2560,4] but got [4,720,2560,6]. [Op:IteratorGetNext]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-8bcf5d4c9447>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdsTrain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mshowDatapoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    745\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 747\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    748\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    737\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_compatible_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 739\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_compatible_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m    128\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# Suppress StopIteration *unless* it's the same exception that\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mexecution_mode\u001b[0;34m(mode)\u001b[0m\n\u001b[1;32m   2114\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2115\u001b[0m       \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecutor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecutor_old\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2116\u001b[0;31m       \u001b[0mexecutor_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/eager/executor.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     67\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;34m\"\"\"Waits for ops dispatched in this executor to finish.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0mpywrap_tfe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFE_ExecutorWaitForAllPendingNodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mclear_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Incompatible shapes at component 0: expected [?,720,2560,4] but got [4,720,2560,6]."
     ]
    }
   ],
   "source": [
    "for i,j in dsTrain:\n",
    "    showDatapoint(i[0],j[0])\n",
    "    break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense \n",
    "from keras import layers\n",
    "from keras import backend as K\n",
    "from keras import Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsampling_block(x, basename, channels=9):\n",
    "    x = layers.Conv2D (channels, 1, padding='same', name = basename+\"expand\")(x)\n",
    "    x = layers.BatchNormalization(axis=-1, epsilon=1e-3, momentum=0.999, name=basename+'_expand_BN')(x)\n",
    "    x = layers.ReLU(6., name=basename+'_expand_relu')(x)\n",
    "    \n",
    "    x = layers.ZeroPadding2D(padding=((1,0), (1,0)), name=basename+'_pad')(x)\n",
    "    x = layers.DepthwiseConv2D(kernel_size=3, strides=(2, 2), depth_multiplier=1, activation=None, use_bias=False, padding='valid',name=basename+'_depthwise')(x)\n",
    "    x = layers.BatchNormalization(axis=-1, epsilon=1e-3, momentum=0.999, name=basename+'_depthwise_BN')(x)\n",
    "    x = layers.ReLU(max_value=6, negative_slope=0, threshold=0, name=basename+'_depthwise_relu' )(x)\n",
    "\n",
    "    x = layers.Conv2D (channels, 1, padding='same', name = basename+\"_project\")(x)\n",
    "#     x = layers.BatchNormalization(axis=-1, epsilon=1e-3, momentum=0.999, name=basename+'_project_BN')(x)\n",
    "    return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(x, basename, channels=9):\n",
    "    x = layers.Conv2D (channels, 1, padding='same', name = basename+\"expand\")(x)\n",
    "    x = layers.BatchNormalization(axis=-1, epsilon=1e-3, momentum=0.999, name=basename+'_expand_BN')(x)\n",
    "    x = layers.ReLU(6., name=basename+'_expand_relu')(x)\n",
    "    \n",
    "    x = layers.DepthwiseConv2D(kernel_size=3, depth_multiplier=1, activation=None, use_bias=False, padding='same',name=basename+'_depthwise')(x)\n",
    "    x = layers.BatchNormalization(axis=-1, epsilon=1e-3, momentum=0.999, name=basename+'_depthwise_BN')(x)\n",
    "    x = layers.ReLU(max_value=6, negative_slope=0, threshold=0, name=basename+'_depthwise_relu' )(x)\n",
    "\n",
    "    x = layers.Conv2D (channels, 1, padding='same', name = basename+\"_project\")(x)\n",
    "#     x = layers.BatchNormalization(axis=-1, epsilon=1e-3, momentum=0.999, name=basename+'_project_BN')(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n",
    "from keras.optimizers import Adam, RMSprop, SGD\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (IMG_HEIGHT, IMG_WIDTH, CHANNELS)\n",
    "\n",
    "model = Sequential()\n",
    "inl = tf.keras.Input(shape=input_shape)\n",
    "x = layers.ZeroPadding2D(padding=((1,0), (1,0)),name=\"Conv1_pad\")(inl)\n",
    "x = layers.Conv2D (9, kernel_size=3, strides=(2, 2), padding='valid', name = \"Conv1\")(x)\n",
    "x = layers.BatchNormalization(axis=-1, epsilon=1e-3, momentum=0.999, name=\"bn1_Conv1\")(x)\n",
    "x = layers.ReLU(6., name='Conv1_relu')(x)\n",
    "x = layers.DepthwiseConv2D(kernel_size=3, depth_multiplier=1, activation=None, use_bias=False, padding='same',name=\"expanded_conv_depthwise\")(x)\n",
    "x = layers.BatchNormalization(axis=-1, epsilon=1e-3, momentum=0.999, name=\"expanded_conv_depthwise_BN\")(x)\n",
    "x = layers.ReLU(6., name='expanded_conv_depthwise_relu')(x)\n",
    "x = layers.Conv2D (9, 1, padding='same', name = \"expanded_conv_project\")(x)\n",
    "expanded_conv_project_BN = layers.BatchNormalization(axis=-1, epsilon=1e-3, momentum=0.999, name=\"expanded_conv_project_BN\")(x)\n",
    "x = downsampling_block(expanded_conv_project_BN,\"block_1\",9)\n",
    "bn_1_layer = layers.BatchNormalization(axis=-1, epsilon=1e-3, momentum=0.999, name=\"block_1_project_BN\")(x)\n",
    "x = conv_block(bn_1_layer,\"block_2\",9)\n",
    "bn_2_layer = layers.BatchNormalization(axis=-1, epsilon=1e-3, momentum=0.999, name=\"block_2_project_BN\")(x)\n",
    "add_2 = layers.Add(name=\"block_2_add\")([bn_1_layer,bn_2_layer])\n",
    "x = downsampling_block(add_2,\"block_3\",9)\n",
    "bn_3_layer = layers.BatchNormalization(axis=-1, epsilon=1e-3, momentum=0.999, name=\"block_3_project_BN\")(x)\n",
    "x = conv_block(bn_3_layer,\"block_4\",9)\n",
    "bn_4_layer = layers.BatchNormalization(axis=-1, epsilon=1e-3, momentum=0.999, name=\"block_4_project_BN\")(x)\n",
    "add_4 = layers.Add(name=\"block_4_add\")([bn_3_layer,bn_4_layer])\n",
    "x = conv_block(add_4,\"block_5\",9)\n",
    "bn_5_layer = layers.BatchNormalization(axis=-1, epsilon=1e-3, momentum=0.999, name=\"block_5_project_BN\")(x)\n",
    "add_5 = layers.Add(name=\"block_5_add\")([add_4,bn_5_layer])\n",
    "x = downsampling_block(add_5,\"block_6\",9)\n",
    "bn_6_layer = layers.BatchNormalization(axis=-1, epsilon=1e-3, momentum=0.999, name=\"block_6_project_BN\")(x)\n",
    "x = conv_block(bn_6_layer,\"block_7\",9)\n",
    "bn_7_layer = layers.BatchNormalization(axis=-1, epsilon=1e-3, momentum=0.999, name=\"block_7_project_BN\")(x)\n",
    "add_7 = layers.Add(name=\"block_7_add\")([bn_6_layer,bn_7_layer])\n",
    "x = conv_block(add_7,\"block_8\",9)\n",
    "bn_8_layer = layers.BatchNormalization(axis=-1, epsilon=1e-3, momentum=0.999, name=\"block_8_project_BN\")(x)\n",
    "add_8 = layers.Add(name=\"block_8_add\")([add_7,bn_8_layer])\n",
    "x = conv_block(add_8,\"block_9\",9)\n",
    "bn_9_layer = layers.BatchNormalization(axis=-1, epsilon=1e-3, momentum=0.999, name=\"block_9_project_BN\")(x)\n",
    "add_9 = layers.Add(name=\"block_9_add\")([add_8,bn_9_layer])\n",
    "x = conv_block(add_9,\"block_10\",9)\n",
    "bn_10_layer = layers.BatchNormalization(axis=-1, epsilon=1e-3, momentum=0.999, name=\"block_10_project_BN\")(x)\n",
    "# add_10 = layers.Add(name=\"block_10_add\")([add_9,bn_10_layer])\n",
    "x = conv_block(bn_10_layer,\"block_11\",9)\n",
    "bn_11_layer = layers.BatchNormalization(axis=-1, epsilon=1e-3, momentum=0.999, name=\"block_11_project_BN\")(x)\n",
    "add_11 = layers.Add(name=\"block_11_add\")([bn_10_layer,bn_11_layer])\n",
    "x = conv_block(add_11,\"block_12\",9)\n",
    "bn_12_layer = layers.BatchNormalization(axis=-1, epsilon=1e-3, momentum=0.999, name=\"block_12_project_BN\")(x)\n",
    "add_12 = layers.Add(name=\"block_12_add\")([bn_11_layer,bn_12_layer])\n",
    "x = layers.Conv2D (9, 1, padding='same', name = \"block_13_expand\")(add_12)\n",
    "x = layers.BatchNormalization(axis=-1, epsilon=1e-3, momentum=0.999, name=\"block_13_expand_BN\")(x)\n",
    "x = layers.ReLU(6.,name ='block_13_expand_relu')(x)\n",
    "\n",
    "x = layers.ZeroPadding2D(padding=((1,0), (1,1)), name='block_13_pad')(x)\n",
    "x = layers.DepthwiseConv2D(kernel_size=3, strides=(2, 2), depth_multiplier=1, activation=None, use_bias=False, padding='valid',name='block_13w_depthwise')(x)\n",
    "x = layers.BatchNormalization(axis=-1, epsilon=1e-3, momentum=0.999, name='block_13_depthwise_BN')(x)\n",
    "x = layers.ReLU(max_value=6, negative_slope=0, threshold=0, name='block_13_depthwise_relu' )(x)\n",
    "\n",
    "x = layers.Conv2D (9, 1, padding='same', name = \"block_13_project\")(x)\n",
    "# x = downsampling_block(add_12,\"block_13\",10)\n",
    "bn_13_layer = layers.BatchNormalization(axis=-1, epsilon=1e-3, momentum=0.999, name=\"block_13_project_BN\")(x)\n",
    "x = conv_block(bn_13_layer,\"block_14\",9)\n",
    "bn_14_layer = layers.BatchNormalization(axis=-1, epsilon=1e-3, momentum=0.999, name=\"block_14_project_BN\")(x)\n",
    "add_14 = layers.Add(name=\"block_14_add\")([bn_13_layer,bn_14_layer])\n",
    "x = conv_block(add_14,\"block_15\",9)\n",
    "x = conv_block(x,\"block_16\",9)\n",
    "bn_16_layer = layers.BatchNormalization(axis=-1, epsilon=1e-3, momentum=0.999, name=\"block_16_project_BN\")(x)\n",
    "x = layers.Conv2D (9, 1, padding='same', activation='relu', name = \"block_18_middle_1conv_output\")(bn_16_layer)\n",
    "# x = layers.Conv2D (10, 1, padding='same', activation='relu', name = \"block_18_middle_2conv_output\")(x)\n",
    "# x = layers.Conv2D (10, 1, padding='same', activation='softmax', name = \"block_18_middle_3conv_output\")(x)\n",
    "x = layers.UpSampling2D(interpolation='bilinear', name='block_19_upto16')(bn_16_layer)\n",
    "x = layers.ZeroPadding2D(padding=((1,0), (0,0)))(x)\n",
    "\n",
    "x = layers.Concatenate(name=\"block_19_concat\")([x,add_12])\n",
    "\n",
    "x = layers.ReLU(max_value=None, negative_slope=0, threshold=0 )(x)\n",
    "x = layers.DepthwiseConv2D(kernel_size=3, strides=(2, 2), depth_multiplier=1, activation=None, use_bias=False, padding='valid')(x)\n",
    "x = layers.UpSampling2D(size=(2,2),interpolation='bilinear', name='block_19fs_upto16')(x)\n",
    "x = layers.UpSampling2D(interpolation='bilinear', name='block_20_upto8')(x)\n",
    "x = layers.ZeroPadding2D(padding=((1,0), (1,1)))(x)\n",
    "x = layers.ZeroPadding2D(padding=((1,0), (1,1)))(x)\n",
    "x = layers.Concatenate(name=\"block_20_concat\")([x,add_5])\n",
    "\n",
    "x = layers.UpSampling2D(interpolation='bilinear', name='block_21_upto4')(x)\n",
    "x = layers.Concatenate(name=\"block_21_concat\")([x,add_2])\n",
    "\n",
    "\n",
    "x = layers.UpSampling2D(interpolation='bilinear', name='block_22_upto2')(x)\n",
    "x = layers.Concatenate(name=\"block_22_concat\")([x,expanded_conv_project_BN])\n",
    "\n",
    "\n",
    "x = layers.UpSampling2D(interpolation='bilinear', name='block_23_upto1')(x)\n",
    "\n",
    "x = layers.Conv2D (9, 1, padding='same', activation='softmax', name = \"block_23_conv_output\")(x)\n",
    "\n",
    "\n",
    "predictions = layers.Conv2D (9, 1, padding='same', activation='softmax', name = \"bflock_18_conv_output\")(x)\n",
    "\n",
    "model = keras.Model(inputs=inl,outputs=predictions)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "              metrics=[tf.keras.metrics.Precision()])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(dsTrain, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = model.evaluate(dsTest, verbose=0)\n",
    "print(\"Evaluated Metrics for the Test-Set:\")\n",
    "for name, value in zip(model.metrics_names, metrics):\n",
    "    print(f\"{name}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printX = dsTest.take(1)\n",
    "predictedY = model.predict(printX)\n",
    "m = np.argmax(predictedY, axis=-1)\n",
    "predictedY = tf.one_hot(m, 10)\n",
    "i=5\n",
    "for x,y in printX:\n",
    "    print(\"True Values\")\n",
    "    showDatapoint(x[i],y[i])\n",
    "    print(\"Network results:\")\n",
    "    showDatapoint(x[i],predictedY[i])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
